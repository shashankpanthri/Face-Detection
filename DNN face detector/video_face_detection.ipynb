{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most of the things we need here are the same as we used for images but I will still do everything again.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The things that will be different will start after we make the function to detect faces and put rectange on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing the necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open cv for image processing and importing \n",
    "import cv2\n",
    "\n",
    "# for linear algebra\n",
    "import numpy as np\n",
    "\n",
    "# for visualizing the images \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can now import the main DNN model files and there are two of them:**\n",
    "* The .prototxt file(s) which define the model architecture (i.e., the layers themselves)\n",
    "* The .caffemodel file which contains the weights for the actual layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = 'model/res10_300x300_ssd_iter_140000.caffemodel'\n",
    "config_file = 'model/deploy.prototxt.txt'\n",
    "\n",
    "# for the actual model\n",
    "net = cv2.dnn.readNetFromCaffe(config_file, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that we have the model we just have to make a function to make a rectangle where it detects a face.**\n",
    "\n",
    "The steps we will follow are:\n",
    "1. Take the image and make a copy of it.\n",
    "2. Make a blob out of the image using cv2 function **blobFromImage** : this basically means that we pre-process the image and do some operations on it for it to be sent into the network. The things included in the setting the dimensions of the blob, normalizing it and some other things that are given as the arguments and are explained below:\n",
    "    * the actual image itself.\n",
    "    \n",
    "    * resize : we resize the image to be `(300,300)` so that it is not too large.\n",
    "    \n",
    "    * scaleFactor : After we perform mean subtraction we can optionally scale our images by some factor. This value defaults to `1.0` (i.e., no scaling) but we can supply another value as well. \n",
    "    \n",
    "    * size : Here we supply the spatial size that the Convolutional Neural Network expects. For most current state-of-the-art neural networks this is either `224×224`, `227×227`, or `299×299`.\n",
    "    \n",
    "    *  These are our mean subtraction values. They can be a 3-tuple of the RGB means.\n",
    "    \n",
    "    * swapRB : OpenCV assumes images are in BGR channel order; however, the `mean` value assumes we are using RGB order. To resolve this discrepancy we can swap the R and B channels in image  by setting this value to `True`. By default OpenCV performs this channel swapping for us.\n",
    "\n",
    "3. Feed the blob to the network.\n",
    "4. Now we get detections which is a 4-dimensional matrix which has for every face it detected some values and they are numberwise:\n",
    "    * don't know\n",
    "    * the current frame (this is relevant if it is being given multiple frames)\n",
    "    * no. of detections (no. of faces it detected)\n",
    "    * this is again a number of values in a single dimension:\n",
    "        * class id\n",
    "        * class score\n",
    "        * confidence (how confident it is that it is detecting a face)\n",
    "        * the actual dimensions of the bounding rectangle (x,y,w,h)\n",
    "        \n",
    "5. Now we loop through all the detections and do :\n",
    "    * if the confidence is above a threshold, then:\n",
    "        * scale the normalized bounding box dimensions to be bigger again.\n",
    "        * make the bounding box on top of the image.\n",
    "        * set the text above the box as the cofidence it has for that box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face(img):\n",
    "    \n",
    "    face = img.copy()\n",
    "    \n",
    "    # the original dimensions of the frame\n",
    "    (h, w) = face.shape[:2]\n",
    "    \n",
    "    # make the blob\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(face, (300, 300)), 1.0,\n",
    "    (300, 300), (104.0, 177.0, 123.0))\n",
    "    \n",
    "    # feed it to the network\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    \n",
    "    # loop over the detections\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        \n",
    "        # extract the confidence of the rectangle\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        \n",
    "        # set the threshold for the minimum confidence (you can mess around with this to get better results)\n",
    "        if confidence > 0.3:\n",
    "            \n",
    "            # get the bounding box coordinates and scale them to fit the frame width\n",
    "            box = detections[0,0,i,3:7] * np.array([w, h, w, h])\n",
    "            (X, Y, width, height) = box.astype('int')\n",
    "            \n",
    "            # draw the bounding box on around the face\n",
    "            cv2.rectangle(face, (X, Y), (width, height), (255,0,0), 2 )\n",
    "            \n",
    "            # get the position above the box\n",
    "            y = Y - 10 if Y - 10 > 10 else Y + 10\n",
    "            \n",
    "            # make the confidence into a percentage value\n",
    "            text = \"{:.2f}%\".format(confidence * 100)\n",
    "            \n",
    "            # put the confidence score above the rectange\n",
    "            cv2.putText(face, text, (X, y),cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255, 255, 255), 2)\n",
    "            \n",
    "    return face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we will do the things differently from the image one. Here we are using video which is nothing but a lot of images showing one after the other based on how many frames(images) are set to show in a second.**\n",
    "\n",
    "We can do two types of video i.e. from a webcam(live video) or from a video file already recorded.\n",
    "\n",
    "So the steps we need to do are:\n",
    "1. Make a video capture object using openCV. This will take the video as input. We can either give it a **path to a video** or just put **0 to for it to automatically detect from the default webcam that is present in the computer**.\n",
    "\n",
    "2. Make a while loop that will constantly do a certain things for us until we stop it. The things are:\n",
    "    * get the frames from the video one by one.\n",
    "    \n",
    "    * pass that frame to the face detection function.\n",
    "    \n",
    "    * show the frame using openCV.\n",
    "    \n",
    "    * Make a waitkey to break out of the while loop.\n",
    "    \n",
    "    * release the capture object after breaking out of the loop.\n",
    "    \n",
    "    * destroy all the windows after breaking out of the loop.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the capture object\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# the infinite while loop\n",
    "while True:\n",
    "    \n",
    "    # read frame by frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # pass the frame to the function\n",
    "    frame = detect_face(frame)\n",
    "    \n",
    "    # display the resultant frame\n",
    "    cv2.imshow(\"face\", frame)\n",
    "    \n",
    "    \n",
    "    # make a waitkey that will wait for a second and break out of the loop if \"q\" is pressed.\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "#release the cap object\n",
    "cap.release()\n",
    "# destroy all the windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also write this video file we are reading frame by frame by using a writer object. That will have around 6 lines of code added to this.\n",
    "\n",
    "The steps for that will be same as above with the added lines of:\n",
    "1. Get the frame width and height of the camera.\n",
    "2. Make a writer object with appropriate arguments.\n",
    "3. Write the frame we are reading after processing it.\n",
    "4. Release the writer object after breaking out of the while loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the capture object\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# get the frame width and height\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))   \n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# make the writer object\n",
    "writer = cv2.VideoWriter('face_detection_video.mp4', cv2.VideoWriter_fourcc(*'DIVX'), 20, (width,height))\n",
    "\n",
    "# the infinite while loop\n",
    "while True:\n",
    "    \n",
    "    # read frame by frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # pass the frame to the function\n",
    "    frame = detect_face(frame)\n",
    "    \n",
    "    # write the frame\n",
    "    writer.write(frame)\n",
    "    \n",
    "    # display the resultant frame\n",
    "    cv2.imshow(\"face\", frame)\n",
    "    \n",
    "    \n",
    "    # make a waitkey that will wait for a second and break out of the loop if \"q\" is pressed.\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "#release the cap object\n",
    "cap.release()\n",
    "# release the writer object\n",
    "writer.release()\n",
    "# destroy the windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
